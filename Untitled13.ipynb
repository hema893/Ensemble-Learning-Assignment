{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fea542-b650-4cc8-bffd-9d842cabdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        ENSEMBLE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105fca11-d7cd-45e4-a290-899122ec83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac3a05-915c-4715-931f-c8ff6347fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble Learning in machine learning is a method where multiple models (called base learners) are combined to solve a problem and improve performance.\n",
    "\n",
    "**Key Idea:** The central idea is that a group of models, when combined, performs better than any single model alone because their strengths complement each other and their errors get reduced. This helps improve accuracy, stability, and generalization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9afcd-08f2-4349-91ac-f9f07d6d1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: What is the difference between Bagging and Boosting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ac4fd-1c18-4ab5-8e72-22f32da3cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Bagging (Bootstrap Aggregating):**\n",
    "\n",
    "* Trains multiple models in **parallel** on different random subsets of the data (sampled with replacement).\n",
    "* Final prediction is made by **majority voting (classification)** or **averaging (regression)**.\n",
    "* Aim: mainly reduces **variance** and prevents overfitting (e.g., Random Forest).\n",
    "\n",
    "**Boosting:**\n",
    "\n",
    "* Trains multiple models **sequentially**, where each new model focuses on correcting the errors of the previous one.\n",
    "* Final prediction is a **weighted combination** of all models.\n",
    "* Aim: reduces both **bias and variance**, improves accuracy but can risk overfitting (e.g., AdaBoost, XGBoost).\n",
    "\n",
    "---\n",
    "\n",
    "* **Bagging → Parallel, reduces variance**\n",
    "* **Boosting → Sequential, reduces bias & variance**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ed707-0f15-438a-8e86-bb785d06d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89368411-412e-4b1d-a921-12a4257c95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Bootstrap Sampling** is a statistical technique where multiple new datasets are created by randomly sampling **with replacement** from the original dataset, so some samples may appear multiple times while others may be left out. Each new dataset is of the same size as the original.\n",
    "\n",
    "**Role in Bagging (e.g., Random Forest):**\n",
    "In Bagging, bootstrap sampling is used to create diverse training subsets for each base learner (e.g., decision tree). Since each model is trained on a slightly different dataset, they make different errors. When their outputs are combined (by voting or averaging), the overall variance is reduced and the model becomes more stable and accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58c080-e924-4fcf-952a-3c6ef94bf523",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12baaf-aa0b-46fd-851b-dd528c48ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Out-of-Bag (OOB) samples:**\n",
    "In bootstrap sampling, each model is trained on a random subset of data (with replacement). On average, about **one-third of the data is left out** in each sample — these are called **Out-of-Bag (OOB) samples**.\n",
    "\n",
    "**OOB Score:**\n",
    "OOB samples act like a built-in validation set. Each model is tested on the data points it didn’t see during training, and their predictions are aggregated. The accuracy (or error) computed using these OOB predictions is called the **OOB score**.\n",
    "\n",
    "This provides an **unbiased estimate of model performance** without needing a separate validation/test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1dd914-568d-4cc9-8431-30aa261eeed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: Compare feature importance analysis in a single Decision Tree vs random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be317d8-c378-49b9-a2cb-c8515205f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Feature Importance in a Single Decision Tree:**\n",
    "\n",
    "* Calculated based on how much each feature **reduces impurity** (e.g., Gini impurity, entropy) across the splits where it is used.\n",
    "* Importance is usually biased if a feature has **many categories** or if the dataset is small.\n",
    "* Since it depends on one tree, results can be **unstable** and vary with small changes in data.\n",
    "\n",
    "**Feature Importance in a Random Forest:**\n",
    "\n",
    "* Computed by **averaging feature importance scores** across all trees in the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc661f-b344-4d27-8ce7-393b629e37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 6: Write a Python program to: \n",
    "● Load the Breast Cancer dataset using \n",
    "sklearn.datasets.load_breast_cancer() \n",
    "● Train a Random Forest Classifier \n",
    "● Print the top 5 most important features based on feature importance scores. \n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b4b3cc-68d0-49cf-89a5-9da40f5eb32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Important Features:\n",
      "\n",
      "                 Feature  Importance\n",
      "23            worst area    0.139357\n",
      "27  worst concave points    0.132225\n",
      "7    mean concave points    0.107046\n",
      "20          worst radius    0.082848\n",
      "22       worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Print top 5 features\n",
    "print(\"Top 5 Most Important Features:\\n\")\n",
    "print(feature_importance_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2210c-c14a-4dc9-8ec7-2084f8f77a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Write a Python program to: \n",
    "● Train a Bagging Classifier using Decision Trees on the Iris dataset \n",
    "● Evaluate its accuracy and compare with a single Decision Tree \n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a28b78-bf72-4b8d-bdbe-721c41475f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt_acc = accuracy_score(y_test, dt.fit(X_train, y_train).predict(X_test))\n",
    "\n",
    "# Bagging with Decision Trees\n",
    "bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bag_acc = accuracy_score(y_test, bag.fit(X_train, y_train).predict(X_test))\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", dt_acc)\n",
    "print(\"Bagging Accuracy:\", bag_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaed531-81c7-4891-9306-d89b451f43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Write a Python program to: \n",
    "● Train a Random Forest Classifier \n",
    "● Tune hyperparameters max_depth and n_estimators using GridSearchCV \n",
    "● Print the best parameters and final accuracy \n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd11ec71-6942-4169-89f8-341266c5f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Final Accuracy: 0.935672514619883\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and final accuracy\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e045bf5-a950-40ec-9947-1606bbf77ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Write a Python program to: \n",
    "● Train a Bagging Regressor and a Random Forest Regressor on the California \n",
    "Housing dataset \n",
    "● Compare their Mean Squared Errors (MSE) \n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ef204-91ed-4839-8f00-971cfa8cdc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fd3c1-ced1-4940-ba50-b35ca0118749",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: You are working as a data scientist at a financial institution to predict loan \n",
    "default. You have access to customer demographic and transaction history data. \n",
    "You decide to use ensemble techniques to increase model performance. \n",
    "Explain your step-by-step approach to: \n",
    "● Choose between Bagging or Boosting \n",
    "● Handle overfitting \n",
    "● Select base models \n",
    "● Evaluate performance using cross-validation \n",
    "● Justify how ensemble learning improves decision-making in this real-world \n",
    "context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0164e72-1585-49d2-819f-52bb4e2d1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "As a data scientist predicting **loan default**, I would use the following step-by-step approach with **ensemble learning**:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Choosing Between Bagging or Boosting**\n",
    "\n",
    "* **Bagging (e.g., Random Forest):** Best if the dataset has **high variance**, useful to stabilize noisy predictions and reduce overfitting.\n",
    "* **Boosting (e.g., XGBoost, LightGBM):** Best if the dataset has **high bias**, since it sequentially corrects errors and usually performs better in structured data like finance.\n",
    "  👉 For loan default prediction, I would **start with Boosting** because it captures complex patterns in customer transactions.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Handling Overfitting**\n",
    "\n",
    "* Use **regularization techniques** (e.g., `max_depth`, `learning_rate`, `min_samples_split`).\n",
    "* Apply **early stopping** in boosting.\n",
    "* Use **cross-validation** to tune hyperparameters.\n",
    "* Reduce noise by feature selection or dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Selecting Base Models**\n",
    "\n",
    "* For Bagging → **Decision Trees** (since they are high variance models, bagging stabilizes them).\n",
    "* For Boosting → **Shallow Decision Trees (stumps)** as weak learners, since boosting combines many weak models into a strong one.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Evaluating Performance**\n",
    "\n",
    "* Use **k-fold cross-validation** to assess model stability across different data splits.\n",
    "* Compare metrics: **AUC-ROC, Precision, Recall, F1-score**, since in loan default prediction, **false negatives (predicting non-default but actually default)** are very costly.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Justification of Ensemble Learning in Real-world Context**\n",
    "\n",
    "* Loan default prediction involves **high risk and imbalanced data**.\n",
    "* Ensemble methods improve **accuracy, robustness, and generalization** compared to single models.\n",
    "* Boosting methods (like XGBoost) are widely used in finance due to their ability to capture **non-linear relationships** and reduce bias.\n",
    "* More reliable predictions help banks in **better credit risk management, reduced losses, and informed decision-making**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485d042-87e1-443b-9860-3c0df949a82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      1047\n",
      "           1       0.96      0.75      0.84       453\n",
      "\n",
      "    accuracy                           0.91      1500\n",
      "   macro avg       0.93      0.87      0.89      1500\n",
      "weighted avg       0.92      0.91      0.91      1500\n",
      "\n",
      "AUC: 0.9589914208787431\n",
      "\n",
      "Gradient Boosting Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      1047\n",
      "           1       0.91      0.77      0.84       453\n",
      "\n",
      "    accuracy                           0.91      1500\n",
      "   macro avg       0.91      0.87      0.89      1500\n",
      "weighted avg       0.91      0.91      0.91      1500\n",
      "\n",
      "AUC: 0.9525228182697965\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Step 1: Create synthetic dataset (loan default = binary classification)\n",
    "X, y = make_classification(n_samples=5000, n_features=20, n_informative=10,\n",
    "                           n_redundant=5, weights=[0.7, 0.3], random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Bagging (Random Forest)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# Step 3: Boosting (Gradient Boosting)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluation\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))\n",
    "\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "print(classification_report(y_test, gb_pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, gb.predict_proba(X_test)[:,1]))\n",
    "\n",
    "# Step 5: Cross-validation comparison\n",
    "rf_cv = cross_val_score(rf, X, y, cv=5, scoring='roc_auc').mean()\n",
    "gb_cv = cross_val_score(gb, X, y, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "print(\"\\nCross-validated AUC -> RF:\", rf_cv, \" | GB:\", gb_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a27052-42eb-4fa5-b181-6f6581724f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
